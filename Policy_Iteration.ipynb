{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxQHTMzVqkbB39myg922Nt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrcoding16/Reinforment_Learning/blob/main/Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ckT1TdeOOaHU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class PolicyIteration:\n",
        "    def _init_(self, reward_function, transition_model, gamma, init_policy=None):\n",
        "        self.num_states = transition_model.shape[0]\n",
        "        self.num_actions = transition_model.shape[1]\n",
        "        self.reward_function = np.nan_to_num(reward_function)\n",
        "\n",
        "        self.transition_model = transition_model\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.values = np.zeros(self.num_states)\n",
        "        if init_policy is None:\n",
        "            self.policy = np.random.randint(0, self.num_actions, self.num_states)\n",
        "        else:\n",
        "            self.policy = init_policy\n",
        "\n",
        "    def one_policy_evaluation(self):\n",
        "        delta = 0\n",
        "        for s in range(self.num_states):\n",
        "            temp = self.values[s]\n",
        "            a = self.policy[s]\n",
        "            p = self.transition_model[s, a]\n",
        "            self.values[s] = self.reward_function[s] + self.gamma * np.sum(p * self.values)\n",
        "            delta = max(delta, abs(temp - self.values[s]))\n",
        "        return delta\n",
        "\n",
        "    def run_policy_evaluation(self, tol=1e-3):\n",
        "        epoch = 0\n",
        "        delta = self.one_policy_evaluation()\n",
        "        delta_history = [delta]\n",
        "        while epoch < 500:\n",
        "            delta = self.one_policy_evaluation()\n",
        "            delta_history.append(delta)\n",
        "            if delta < tol:\n",
        "                break\n",
        "        return len(delta_history)\n",
        "\n",
        "    def run_policy_improvement(self):\n",
        "        update_policy_count = 0\n",
        "        for s in range(self.num_states):\n",
        "            temp = self.policy[s]\n",
        "            v_list = np.zeros(self.num_actions)\n",
        "            for a in range(self.num_actions):\n",
        "                p = self.transition_model[s, a]\n",
        "                v_list[a] = np.sum(p * self.values)\n",
        "            self.policy[s] = np.argmax(v_list)\n",
        "            if temp != self.policy[s]:\n",
        "                update_policy_count += 1\n",
        "        return update_policy_count\n",
        "\n",
        "    def train(self, tol=1e-3, plot=True):\n",
        "        epoch = 0\n",
        "        eval_count = self.run_policy_evaluation(tol=tol)\n",
        "        eval_count_history = [eval_count]\n",
        "        policy_change = self.run_policy_improvement()\n",
        "        policy_change_history = [policy_change]\n",
        "        while epoch < 500:\n",
        "            epoch += 1\n",
        "            new_eval_count = self.run_policy_evaluation(tol)\n",
        "            new_policy_change = self.run_policy_improvement()\n",
        "            eval_count_history.append(new_eval_count)\n",
        "            policy_change_history.append(new_policy_change)\n",
        "            if new_policy_change == 0:\n",
        "                break\n",
        "\n",
        "        print(f'# epoch: {len(policy_change_history)}')\n",
        "        print(f'eval count = {eval_count_history}')\n",
        "        print(f'policy change = {policy_change_history}')\n",
        "\n",
        "        if plot is True:\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(3.5, 4), sharex='all', dpi=200)\n",
        "            axes[0].plot(np.arange(len(eval_count_history)), eval_count_history, marker='o', markersize=4, alpha=0.7,\n",
        "                         color='#2ca02c', label='# sweep in \\npolicy evaluation\\n' + r'$\\gamma =$' + f'{self.gamma}')\n",
        "            axes[0].legend()\n",
        "\n",
        "            axes[1].plot(np.arange(len(policy_change_history)), policy_change_history, marker='o',\n",
        "                         markersize=4, alpha=0.7, color='#d62728',\n",
        "                         label='# policy updates in \\npolicy improvement\\n' + r'$\\gamma =$' + f'{self.gamma}')\n",
        "            axes[1].set_xlabel('Epoch')\n",
        "            axes[1].legend()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9-cd1iGPOAH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}